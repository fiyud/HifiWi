{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "103f9e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open npy file\n",
    "\n",
    "import numpy as np\n",
    "data = np.load(r'C:\\Users\\Admin\\OneDrive\\Desktop\\Desktop\\DT-Pose\\data\\wipose\\Test_Amplitude_DWT\\bend_009-frame001.npz', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77def893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['CSI', 'SkeletonPoints'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mat73\n",
    "data_mat = mat73.loadmat(r\"C:\\Users\\Admin\\OneDrive\\Desktop\\Desktop\\DT-Pose\\data\\wipose\\Test\\bend_009-frame001.mat\")\n",
    "data_mat.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "665bdae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "csi = data_mat[\"CSI\"]\n",
    "ske = data_mat['SkeletonPoints']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce2b28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "0.0 54.38532377656736\n"
     ]
    }
   ],
   "source": [
    "print(csi.dtype)\n",
    "print(np.min(csi), np.max(csi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4efa548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54,)\n"
     ]
    }
   ],
   "source": [
    "print(ske.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "545ef50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.37894043e+02 2.01325386e+02 2.02578598e+02 2.87208374e+02\n",
      " 3.54465942e+02 2.00072174e+02 2.70874390e+02 3.54368103e+02\n",
      " 3.51864746e+02 4.49056671e+02 5.51434631e+02 3.36840363e+02\n",
      " 4.39174683e+02 5.38649231e+02 1.22940201e+02 1.18044479e+02\n",
      " 1.40380630e+02 4.34683350e+02 2.80597961e+02 3.17884216e+02\n",
      " 3.40368103e+02 3.27835083e+02 2.93002258e+02 2.95400330e+02\n",
      " 2.62624847e+02 2.70937714e+02 3.25476166e+02 3.15474335e+02\n",
      " 3.42695435e+02 3.02304626e+02 3.04943054e+02 3.30265167e+02\n",
      " 2.95568909e+02 2.83047424e+02 3.32844452e+02 3.22191772e+02\n",
      " 8.27377081e-01 6.61848128e-01 6.84330463e-01 8.88884127e-01\n",
      " 8.27618122e-01 6.39365792e-01 2.67183959e-01 3.39344501e-01\n",
      " 4.56409603e-01 6.17439151e-01 5.57545483e-01 3.19272220e-01\n",
      " 5.87602317e-01 6.24153256e-01 8.73584092e-01 2.30846509e-01\n",
      " 8.88585567e-01 9.17825289e-03]\n"
     ]
    }
   ],
   "source": [
    "print(ske)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd7214e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 90, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['CSI'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92aa530c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[137.89404297, 201.32538605, 202.57859802, 287.20837402,\n",
       "        354.46594238, 200.07217407, 270.87438965, 354.36810303,\n",
       "        351.86474609, 449.05667114, 551.43463135, 336.84036255,\n",
       "        439.17468262, 538.64923096, 122.94020081, 118.04447937,\n",
       "        140.38063049, 434.68334961],\n",
       "       [280.59796143, 317.88421631, 340.36810303, 327.83508301,\n",
       "        293.0022583 , 295.40032959, 262.62484741, 270.93771362,\n",
       "        325.47616577, 315.47433472, 342.69543457, 302.30462646,\n",
       "        304.9430542 , 330.26516724, 295.56890869, 283.04742432,\n",
       "        332.8444519 , 322.19177246]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['SkeletonPoints']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef9e0244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 18)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['SkeletonPoints'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0b93f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open(r\"C:\\Users\\Admin\\OneDrive\\Desktop\\Desktop\\DT-Pose\\config\\wipose\\pose_config.yaml\", 'r') as fd:\n",
    "        config = yaml.load(fd, Loader=yaml.FullLoader)\n",
    "\n",
    "from model.hpeli.hpeli import hpelinet, hpeli_weights_init\n",
    "model = hpelinet(num_keypoints=18, num_coor=2, subcarrier_num=90, num_person= 5, dataset=config['dataset_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d426a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open(r\"C:\\Users\\Admin\\OneDrive\\Desktop\\Desktop\\DT-Pose\\config\\wipose\\pose_config.yaml\", 'r') as fd:\n",
    "        config = yaml.load(fd, Loader=yaml.FullLoader)\n",
    "\n",
    "from model.hpeli.hpeli import hpelinet, hpeli_weights_init\n",
    "model = hpelinet(num_keypoints=18, num_coor=2, subcarrier_num=90, num_person= 2, dataset=config['dataset_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a59157ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[ 0.1430, -0.0485],\n",
      "         [-0.1786, -0.0842],\n",
      "         [-0.0368, -0.1456],\n",
      "         [-0.0309,  0.0674],\n",
      "         [-0.0487,  0.0646],\n",
      "         [ 0.1332,  0.1707],\n",
      "         [-0.0266, -0.0090],\n",
      "         [-0.0544,  0.0148],\n",
      "         [-0.0764, -0.1523],\n",
      "         [-0.0694,  0.0949],\n",
      "         [ 0.1254, -0.0603],\n",
      "         [-0.0474, -0.0040],\n",
      "         [ 0.1229,  0.0831],\n",
      "         [ 0.0346,  0.0950],\n",
      "         [-0.0663, -0.0568],\n",
      "         [-0.0953, -0.0028],\n",
      "         [-0.1764,  0.0256],\n",
      "         [ 0.1634, -0.0701]]]), tensor([[0.3829, 0.4231, 0.3681, 0.3800, 0.4168, 0.4183, 0.4222, 0.3989, 0.4158,\n",
      "         0.3998, 0.4038, 0.3916, 0.3715, 0.3520, 0.3898, 0.3973, 0.3718, 0.3910,\n",
      "         0.4301, 0.4251, 0.3975, 0.3875, 0.3888, 0.3886, 0.4063, 0.3999, 0.4096,\n",
      "         0.4161, 0.3912, 0.4037, 0.4389, 0.4369, 0.4119, 0.4054, 0.4109, 0.4299,\n",
      "         0.3926, 0.4264, 0.4192, 0.4287, 0.4199, 0.4120, 0.3759, 0.4133, 0.3776,\n",
      "         0.3659, 0.3956, 0.3945, 0.3615, 0.4240, 0.3958, 0.4221, 0.4036, 0.3806,\n",
      "         0.3932, 0.3911, 0.4122, 0.3849, 0.3953, 0.4114, 0.4205, 0.4111, 0.4058,\n",
      "         0.4122, 0.3835, 0.4003, 0.3941, 0.3910, 0.3899, 0.4241, 0.4190, 0.3729,\n",
      "         0.3886, 0.3721, 0.3999, 0.3739, 0.3685, 0.4290, 0.4236, 0.3775, 0.3789,\n",
      "         0.4099, 0.3972, 0.4082, 0.4003, 0.3935, 0.4158, 0.3776, 0.4133, 0.3972,\n",
      "         0.4153, 0.4008, 0.4066, 0.4269, 0.3908, 0.3809, 0.4149, 0.3576, 0.3982,\n",
      "         0.4096, 0.3950, 0.3854, 0.4243, 0.4162, 0.4096, 0.3741, 0.3926, 0.4063,\n",
      "         0.4042, 0.3876, 0.4138, 0.3707, 0.3837, 0.4105, 0.3901, 0.3983, 0.4092,\n",
      "         0.4040, 0.4140, 0.3606, 0.3860, 0.3869, 0.4199, 0.4161, 0.3827, 0.4085,\n",
      "         0.3789, 0.3887]]))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "input_tensor = torch.randn(1, 3, 90, 5)  # Example input tensor with batch size 1\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7aa22dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.model import *\n",
    "model = MAE_ViT(image_size=(90, 5),\n",
    "                        patch_size=(2,1),\n",
    "                        encoder_layer=4,\n",
    "                        encoder_head=4,\n",
    "                        decoder_layer=2,\n",
    "                        decoder_head=4,\n",
    "                        emb_dim=256,\n",
    "                        input_dim=3)\n",
    "model = ViT_Pose_Decoder(model.encoder, keypoints=18, coor_num=2, token_num=45*5, dataset=\"wipose\", num_person=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "515585f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[ 0.3743, -0.1117],\n",
      "         [ 0.2758,  0.0624],\n",
      "         [ 0.3030,  0.0434],\n",
      "         [ 0.3318, -0.0961],\n",
      "         [ 0.2565, -0.1185],\n",
      "         [ 0.2993, -0.0053],\n",
      "         [ 0.3386, -0.0615],\n",
      "         [ 0.2949, -0.2213],\n",
      "         [ 0.3466, -0.0104],\n",
      "         [ 0.2324, -0.0734],\n",
      "         [ 0.3185, -0.2200],\n",
      "         [ 0.3022, -0.0048],\n",
      "         [ 0.3093,  0.0195],\n",
      "         [ 0.3543, -0.2373],\n",
      "         [ 0.3659, -0.0682],\n",
      "         [ 0.3139, -0.0666],\n",
      "         [ 0.3882, -0.1378],\n",
      "         [ 0.2778, -0.2232]]]), tensor([[ 0.5104,  0.1795,  0.1546,  0.0659,  0.5399, -0.1659, -0.7188,  0.4902,\n",
      "          0.3245, -0.6873, -0.5966, -0.6297,  0.3480,  0.5812,  0.4775, -1.7900,\n",
      "          0.4202,  0.3781,  0.1919,  0.2758, -0.8410, -0.3256,  0.0426, -0.2873,\n",
      "          0.1674,  0.4991, -0.6622, -0.0350,  0.2637,  0.4684, -0.2940,  0.4113,\n",
      "         -0.2910, -0.7472,  0.9100,  0.1319, -0.2783,  0.6544, -1.0215, -0.2935,\n",
      "          1.0550, -0.1111,  0.5830, -0.8829, -0.3210,  0.4826, -0.2351,  1.3188,\n",
      "         -0.7380, -1.1952,  0.9891, -0.4609, -0.7771, -0.3983, -0.1083, -0.9360,\n",
      "         -0.9933,  0.0172, -0.1683, -0.2964,  0.6972,  0.8381,  0.6278, -0.4205,\n",
      "          0.2005, -0.3606, -0.3348,  0.4823, -0.0302,  0.5416,  1.1203, -0.0598,\n",
      "         -0.5195,  0.3026, -0.2317,  0.3709, -0.1972,  0.0515,  0.1916, -0.5444,\n",
      "          0.8346, -0.3654,  0.5213,  0.4437,  0.4027, -0.3192, -0.7481,  1.0174,\n",
      "          0.3698,  0.2995, -0.2597,  1.0104, -0.1810, -0.2152,  0.9610, -0.0674,\n",
      "          0.5093, -0.2101, -0.2782, -0.9523, -0.3013, -0.7959, -0.3596, -0.2979,\n",
      "          0.7025,  0.0222,  0.1581,  0.3996, -0.5652,  0.4486, -0.3064, -0.5887,\n",
      "          0.3622,  0.3037,  0.4612,  0.6716,  0.3911, -0.1877, -0.8047,  1.0500,\n",
      "         -0.0580, -0.0480,  0.1276, -0.6908, -0.2799, -0.9763, -0.4315, -0.1782,\n",
      "          0.9304,  0.2892,  0.5308, -0.4020, -0.0706,  0.1085,  0.4989, -0.3727,\n",
      "         -0.4871, -0.5897,  0.1592, -0.3085,  0.4928, -0.0424,  0.6189, -0.4193,\n",
      "          0.9410, -0.0412, -1.0590,  0.3146, -1.0803, -0.3361, -1.2668,  0.1877,\n",
      "          0.4948,  0.0589, -0.5790, -0.2706, -0.9767,  0.1588,  0.6648, -0.0219,\n",
      "          0.0925,  0.2462, -0.5021,  0.5540, -0.1725, -0.4719, -0.4348, -0.2766,\n",
      "         -0.0577, -0.3253, -0.3136,  0.3436,  0.4351, -0.5286,  0.3419, -0.5692,\n",
      "         -0.1830,  0.0770, -0.0464, -0.0883, -0.1236,  0.0664,  0.9855,  0.6454,\n",
      "         -0.3093, -0.1455, -0.6672, -0.2129,  0.7351, -0.5590,  0.4021,  0.0384,\n",
      "          0.6618,  1.2571, -0.9556,  0.6900, -0.1273,  0.0900, -1.0338, -0.3260,\n",
      "         -0.3444, -0.6562,  0.0090, -0.0585,  0.2184,  0.2338, -0.0240, -0.8228,\n",
      "          0.4512, -0.0961,  0.0200,  0.1440, -0.5591,  0.4559,  0.0432, -0.3106,\n",
      "          0.4719,  0.6213, -0.5240, -0.6130, -0.1799,  0.8615, -0.2162,  0.4488,\n",
      "          1.0740,  0.7162, -0.0765,  0.5337, -1.5689,  0.0244, -1.5013, -0.2950,\n",
      "          0.2390,  0.3212, -0.3543,  0.0663,  0.0934, -0.7176,  1.1510,  0.0999,\n",
      "          0.3262, -0.2497,  0.4922, -0.9120,  0.6205,  0.2690,  0.9010,  0.6040,\n",
      "          0.7401, -0.0925,  0.6807,  0.6293, -0.1173,  0.7193, -0.7289,  0.0085]]))\n"
     ]
    }
   ],
   "source": [
    "inp = torch.randn(1, 3, 90, 5)\n",
    "with torch.no_grad():\n",
    "    out = model(inp)    \n",
    "    print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68a42798",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\dfine\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\anaconda3\\envs\\dfine\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from model.metafi.mynetwork import metafinet, metafi_weights_init\n",
    "model = metafinet(num_keypoints=18, num_coor=2, dataset=\"wipose\", num_person=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "987bf4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 18, 2])\n"
     ]
    }
   ],
   "source": [
    "inp = torch.randn(1, 3, 90, 5)\n",
    "out = model(inp)\n",
    "print(out[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d36a44d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3539,  1.6438],\n",
       "         [-1.5514,  0.8277],\n",
       "         [-0.7312,  1.5237],\n",
       "         [ 0.1404,  0.3777],\n",
       "         [-0.4384, -1.2197],\n",
       "         [-0.0387, -1.1263],\n",
       "         [ 0.5702, -0.1476],\n",
       "         [-0.7111, -0.5480],\n",
       "         [-0.3910, -1.3520],\n",
       "         [ 2.7352,  0.3801],\n",
       "         [ 1.5579,  0.0488],\n",
       "         [ 1.2344, -1.1225],\n",
       "         [-0.1814, -1.1854],\n",
       "         [-0.9136, -1.2480],\n",
       "         [ 0.5247,  0.3190],\n",
       "         [-0.6755,  1.2002],\n",
       "         [-0.8356,  0.8232],\n",
       "         [ 0.0591,  0.8054]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "19e83f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 5245059 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# calculate number of parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {count_parameters(model)} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf703f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.shape torch.Size([1, 5, 18, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "# -------------------------\n",
    "# Simple Hypergraph Convolution (HGNN) implementation (based on HGNN paper)\n",
    "# H: incidence matrix (num_nodes x num_hyperedges)\n",
    "# X: node features (B, num_nodes, D)\n",
    "# -------------------------\n",
    "class HypergraphConv(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, use_bias=True):\n",
    "        super().__init__()\n",
    "        self.theta = nn.Linear(in_dim, out_dim, bias=use_bias)\n",
    "\n",
    "    def forward(self, X, H):\n",
    "        H = H.float()\n",
    "        deg_v = torch.clamp(H.sum(1), min=1e-6)\n",
    "        deg_e = torch.clamp(H.sum(0), min=1e-6)\n",
    "        D_v_inv_sqrt = torch.diag(1.0 / torch.sqrt(deg_v))\n",
    "        D_e_inv = torch.diag(1.0 / deg_e)\n",
    "        A = D_v_inv_sqrt @ H @ D_e_inv @ H.t() @ D_v_inv_sqrt\n",
    "        A = A / (A.sum(1, keepdim=True) + 1e-8)  # normalize row\n",
    "        AX = torch.matmul(A, X)\n",
    "        out = self.theta(AX)\n",
    "        out = F.layer_norm(out, out.shape[-1:])\n",
    "        return out + X if X.shape[-1] == out.shape[-1] else out\n",
    "\n",
    "# CSI Encoder\n",
    "class CSI_Encoder(nn.Module):\n",
    "    def __init__(self, in_channels=3, emb_dim=256):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, emb_dim, kernel_size=3, padding=1)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((None, 1))  # compress frequency dimension\n",
    "        self.norm = nn.LayerNorm(emb_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)  # B x emb_dim x H x 1\n",
    "        x = x.squeeze(-1) # B x emb_dim x H\n",
    "        x = rearrange(x, 'b d t -> b t d')  # B x T x D\n",
    "        x = self.norm(x)\n",
    "        return x  # memory\n",
    "\n",
    "\n",
    "# Decoder with per-person cross-attention + HGNN refine\n",
    "class MultiPersonDecoderPerPerson(nn.Module):\n",
    "    def __init__(self, emb_dim=256, num_keypoints=17, num_person=2, num_layers=3, nhead=8, hgnn_hidden=256, coor_num=3):\n",
    "        super().__init__()\n",
    "        self.num_person = num_person\n",
    "        self.num_keypoints = num_keypoints\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        # queries\n",
    "        self.person_queries = nn.Parameter(torch.randn(num_person, emb_dim))\n",
    "        self.keypoint_queries = nn.Parameter(torch.randn(num_keypoints, emb_dim))\n",
    "\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=emb_dim, nhead=nhead, batch_first=True)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # after decoder we have embeddings per keypoint; apply HGNN (works on nodes = keypoints)\n",
    "        self.hgnn1 = HypergraphConv(in_dim=emb_dim, out_dim=hgnn_hidden)\n",
    "        self.hgnn2 = HypergraphConv(in_dim=hgnn_hidden, out_dim=emb_dim)\n",
    "\n",
    "        # final head: embedding -> coord\n",
    "        self.fc_out = nn.Linear(emb_dim, coor_num)\n",
    "\n",
    "    def forward(self, memory, H):\n",
    "        \"\"\"\n",
    "        memory: (B, T, D)\n",
    "        H: incidence matrix (N=num_keypoints, E) torch tensor (shared)\n",
    "        returns: coords (B, P, K, 3)\n",
    "        \"\"\"\n",
    "        B, T, D = memory.shape\n",
    "        device = memory.device\n",
    "\n",
    "        person_q = self.person_queries.unsqueeze(0).repeat(B, 1, 1)  # B x P x D\n",
    "        key_q = self.keypoint_queries.unsqueeze(0).repeat(B, 1, 1)   # B x K x D\n",
    "\n",
    "        all_person_outputs = []\n",
    "        # loop per person to get isolated cross-attention\n",
    "        for p in range(self.num_person):\n",
    "            # construct tgt for this person: (B, K, D)\n",
    "            # broadcast person embedding and add keypoint queries (simple interaction)\n",
    "            per_person_query = person_q[:, p:p+1, :]  # (B,1,D)\n",
    "            tgt = per_person_query + key_q            # (B,K,D)\n",
    "            # TransformerDecoder expects tgt (B, tgt_len, D) and memory (B, src_len, D)\n",
    "            out_emb = self.transformer_decoder(tgt=tgt, memory=memory)  # (B, K, D)\n",
    "\n",
    "            # Hypergraph refine: HGNN expects (B, N, D) where N = K\n",
    "            out_ref1 = self.hgnn1(out_emb, H)    # (B, K, hgnn_hidden)\n",
    "            out_ref2 = self.hgnn2(out_ref1, H)   # (B, K, D)\n",
    "\n",
    "            coords = self.fc_out(out_ref2)       # (B, K, 3)\n",
    "            all_person_outputs.append(coords.unsqueeze(1))  # add person dim\n",
    "\n",
    "        # concat persons: list of (B,1,K,3) -> (B,P,K,3)\n",
    "        all_person_outputs = torch.cat(all_person_outputs, dim=1)\n",
    "        return all_person_outputs  # (B, P, K, 3)\n",
    "\n",
    "\n",
    "class CSI_HPE_withHGNN(nn.Module):\n",
    "    def __init__(self, emb_dim=128, num_keypoints=17, num_person=1, coor_num=3):\n",
    "        super().__init__()\n",
    "        self.encoder = CSI_Encoder(in_channels=3, emb_dim=emb_dim)\n",
    "        self.decoder = MultiPersonDecoderPerPerson(emb_dim=emb_dim, num_keypoints=num_keypoints, num_person=num_person, coor_num=coor_num)\n",
    "\n",
    "    def forward(self, x, H):\n",
    "        # x: (B, C, H_time, W_freq)\n",
    "        memory = self.encoder(x)  # (B, T, D)\n",
    "        coords = self.decoder(memory, H)  # (B, P, K, 3)\n",
    "        return coords\n",
    "    \n",
    "def init_csi_hpe_weights(m):\n",
    "    # Conv2d\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    # Linear\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    # BatchNorm / LayerNorm\n",
    "    elif isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.LayerNorm)):\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    # HypergraphConv (Linear theta)\n",
    "    elif isinstance(m, HypergraphConv):\n",
    "        nn.init.xavier_normal_(m.theta.weight)\n",
    "        if m.theta.bias is not None:\n",
    "            nn.init.constant_(m.theta.bias, 0)\n",
    "    \n",
    "    # TransformerDecoderLayer (Linear + LayerNorm)\n",
    "    elif isinstance(m, nn.TransformerDecoderLayer):\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                if param.dim() > 1:\n",
    "                    nn.init.xavier_normal_(param)\n",
    "                else:\n",
    "                    nn.init.constant_(param, 1)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(param, 0)\n",
    "\n",
    "# -------------------------\n",
    "# Quick test\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    B = 3; C = 3; T = 90; W = 5\n",
    "    P = 5; K = 18; D = 256; CN = 2\n",
    "    model = CSI_HPE_withHGNN(emb_dim=D, num_keypoints=K, num_person=P, coor_num=CN)\n",
    "    inp = torch.randn(1,3, 90, 5)\n",
    "\n",
    "    # example incidence H: N x E\n",
    "    # simplest: per-person hyperedge connecting all K joints (E = P)\n",
    "    # Here we'll create H for a single person (we can reuse same H for all persons or include person hyperedges)\n",
    "    # For demonstration let's create E = K (each hyperedge = single joint) + 1 global hyperedge (all joints)\n",
    "    # But standard HGNN expects meaningful H; simplest: create one hyperedge connecting all joints:\n",
    "    H = torch.ones(K, 1)  # every joint belongs to the single hyperedge (all-joints)\n",
    "    out = model(inp, H)\n",
    "    print(\"out.shape\", out.shape)  # (B, P, K, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8825cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.8023,  0.0514],\n",
       "          [-1.7860,  0.2651],\n",
       "          [-1.9362, -0.2768],\n",
       "          [-2.0887, -0.2393],\n",
       "          [-1.5071, -0.2198],\n",
       "          [-1.4814,  0.0521],\n",
       "          [-1.6043, -0.2418],\n",
       "          [-1.8751,  0.2762],\n",
       "          [-2.0711, -0.1256],\n",
       "          [-2.1949, -0.2065],\n",
       "          [-1.9869, -0.0716],\n",
       "          [-1.7747,  0.3363],\n",
       "          [-2.2216,  0.0504],\n",
       "          [-1.5461, -0.3080],\n",
       "          [-1.9417, -0.2109],\n",
       "          [-1.5849,  0.0308],\n",
       "          [-1.5969,  0.2516],\n",
       "          [-1.4472, -0.3145]]]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "73f67a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 1, 90, 5)  # B x C x H x W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0d833fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "317798f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 90, 5])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f726640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person 0 query shape: torch.Size([2, 1, 8])\n",
      "Keypoint query shape: torch.Size([2, 5, 8])\n",
      "tgt shape: torch.Size([2, 5, 8])\n",
      "----------------------------------------\n",
      "Person 1 query shape: torch.Size([2, 1, 8])\n",
      "Keypoint query shape: torch.Size([2, 5, 8])\n",
      "tgt shape: torch.Size([2, 5, 8])\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Giả sử\n",
    "B = 2           # batch size\n",
    "K = 5           # số keypoints\n",
    "emb_dim = 8     # embedding dimension\n",
    "num_person = 2\n",
    "\n",
    "# Tạo keypoint queries (B x K x emb_dim)\n",
    "key_q = torch.randn(B, K, emb_dim)\n",
    "\n",
    "# Tạo person queries (num_person x emb_dim)\n",
    "person_q = torch.randn(num_person, emb_dim)\n",
    "\n",
    "# Lặp qua từng person\n",
    "for p in range(num_person):\n",
    "    # Chọn person query, shape (B, 1, emb_dim)\n",
    "    per_person_query = person_q[p].unsqueeze(0).unsqueeze(1).repeat(B, 1, 1)\n",
    "\n",
    "    # Cộng với keypoint queries (broadcasting)\n",
    "    tgt = per_person_query + key_q\n",
    "\n",
    "    print(f\"Person {p} query shape: {per_person_query.shape}\")\n",
    "    print(f\"Keypoint query shape: {key_q.shape}\")\n",
    "    print(f\"tgt shape: {tgt.shape}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "155b4309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ninja in c:\\users\\admin\\anaconda3\\envs\\dfine\\lib\\site-packages (1.11.1.1)\n",
      "Requirement already satisfied: cmake in c:\\users\\admin\\anaconda3\\envs\\dfine\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\anaconda3\\envs\\dfine\\lib\\site-packages (80.9.0)\n",
      "Requirement already satisfied: wheel in c:\\users\\admin\\anaconda3\\envs\\dfine\\lib\\site-packages (0.45.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~treamlit (C:\\Users\\Admin\\anaconda3\\envs\\dfine\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~treamlit (C:\\Users\\Admin\\anaconda3\\envs\\dfine\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~treamlit (C:\\Users\\Admin\\anaconda3\\envs\\dfine\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: packaging in c:\\users\\admin\\anaconda3\\envs\\dfine\\lib\\site-packages (24.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~treamlit (C:\\Users\\Admin\\anaconda3\\envs\\dfine\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~treamlit (C:\\Users\\Admin\\anaconda3\\envs\\dfine\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~treamlit (C:\\Users\\Admin\\anaconda3\\envs\\dfine\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\OneDrive\\Desktop\\Desktop\\DT-Pose\\mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'mamba'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///C:/Users/Admin/OneDrive/Desktop/Desktop/DT-Pose/mamba\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Checking if build backend supports build_editable: started\n",
      "  Checking if build backend supports build_editable: finished with status 'done'\n",
      "  Getting requirements to build editable: started\n",
      "  Getting requirements to build editable: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~treamlit (C:\\Users\\Admin\\anaconda3\\envs\\dfine\\Lib\\site-packages)\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Getting requirements to build editable did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [29 lines of output]\n",
      "      C:\\Users\\Admin\\AppData\\Local\\Temp\\pip-build-env-4z9_l_5u\\overlay\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "        cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n",
      "      \n",
      "      \n",
      "      torch.__version__  = 2.9.1+cpu\n",
      "      \n",
      "      \n",
      "      <string>:118: UserWarning: mamba_ssm was requested, but nvcc was not found.  Are you sure your environment has nvcc available?  If you're installing within a container from https://hub.docker.com/r/pytorch/pytorch, only images whose names contain 'devel' will provide nvcc.\n",
      "      Traceback (most recent call last):\n",
      "        File \"C:\\Users\\Admin\\anaconda3\\envs\\dfine\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\n",
      "          main()\n",
      "        File \"C:\\Users\\Admin\\anaconda3\\envs\\dfine\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\n",
      "          json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Admin\\anaconda3\\envs\\dfine\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 132, in get_requires_for_build_editable\n",
      "          return hook(config_settings)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\pip-build-env-4z9_l_5u\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 473, in get_requires_for_build_editable\n",
      "          return self.get_requires_for_build_wheel(config_settings)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\pip-build-env-4z9_l_5u\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 331, in get_requires_for_build_wheel\n",
      "          return self._get_build_requires(config_settings, requirements=[])\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\pip-build-env-4z9_l_5u\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 301, in _get_build_requires\n",
      "          self.run_setup()\n",
      "        File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\pip-build-env-4z9_l_5u\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 317, in run_setup\n",
      "          exec(code, locals())\n",
      "        File \"<string>\", line 175, in <module>\n",
      "      NameError: name 'bare_metal_version' is not defined\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "× Getting requirements to build editable did not run successfully.\n",
      "│ exit code: 1\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install ninja cmake setuptools wheel\n",
    "!pip install packaging\n",
    "\n",
    "# clone mamba\n",
    "!git clone https://github.com/state-spaces/mamba.git\n",
    "%cd mamba\n",
    "\n",
    "# build selective_scan\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdd8c3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from packaging.version import parse\n",
    "\n",
    "def get_cuda_bare_metal_version(cuda_dir):\n",
    "    nvcc_path = os.path.join(cuda_dir, \"bin\", \"nvcc.exe\")  # Windows fix\n",
    "\n",
    "    if not os.path.exists(nvcc_path):\n",
    "        raise RuntimeError(f\"nvcc not found at {nvcc_path}\")\n",
    "\n",
    "    raw_output = subprocess.check_output(\n",
    "        [nvcc_path, \"-V\"], universal_newlines=True\n",
    "    )\n",
    "    output = raw_output.split()\n",
    "    release_idx = output.index(\"release\") + 1\n",
    "    bare_metal_ver = parse(output[release_idx].split(\",\")[0])\n",
    "\n",
    "    return raw_output, bare_metal_ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18fdab62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.8\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7b3bdd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_HOME: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\n",
      "CUDA_PATH: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"CUDA_HOME:\", os.environ.get(\"CUDA_HOME\"))\n",
    "print(\"CUDA_PATH:\", os.environ.get(\"CUDA_PATH\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acb5c20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "rand = torch.randn(32,1,14,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "836cf48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rand.reshape((-1, 14, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcdabc70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 14, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d97cea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.load(r'C:\\Users\\Admin\\OneDrive\\Desktop\\Desktop\\DT-Pose\\data\\person_in_wifi_3d\\test_data\\keypoint\\S51_10_333.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9f44dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 14, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3452f94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all-person: Traning sample 89945.\n",
      "all-person: Test sample 7824.\n"
     ]
    }
   ],
   "source": [
    "from feeder.person_in_wifi_3d import PersonInWif3D, piw3_make_dataloader\n",
    "import yaml\n",
    "import torch\n",
    "dataset_root = r\"C:\\Users\\Admin\\OneDrive\\Desktop\\Desktop\\DT-Pose\\data\\person_in_wifi_3d\"\n",
    "with open(r\"C:\\Users\\Admin\\OneDrive\\Desktop\\Desktop\\DT-Pose\\config\\person_in_wifi_3d\\pose_config.yaml\", 'r') as fd:\n",
    "    config = yaml.load(fd, Loader=yaml.FullLoader)\n",
    "\n",
    "train_dataset = PersonInWif3D('training', dataset_root, config['setting'])\n",
    "val_dataset = PersonInWif3D('validation', dataset_root, config['setting'])\n",
    "rng_generator = torch.manual_seed(config['init_rand_seed'])\n",
    "train_loader = piw3_make_dataloader(train_dataset, is_training=True, generator=rng_generator, batch_size=32)\n",
    "val_loader = piw3_make_dataloader(val_dataset, is_training=False, generator=rng_generator, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6642120",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"c:\\Users\\Admin\\anaconda3\\envs\\dfine\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\anaconda3\\envs\\dfine\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\anaconda3\\envs\\dfine\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"c:\\Users\\Admin\\OneDrive\\Desktop\\Desktop\\DT-Pose\\feeder\\person_in_wifi_3d.py\", line 129, in __getitem__\n    pose_data = np.array(np.load(item['keypoint_path']))\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\anaconda3\\envs\\dfine\\Lib\\site-packages\\numpy\\lib\\npyio.py\", line 427, in load\n    fid = stack.enter_context(open(os_fspath(file), \"rb\"))\n                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Admin\\\\OneDrive\\\\Desktop\\\\Desktop\\\\DT-Pose\\\\data\\\\person_in_wifi_3d\\\\train_data\\\\keypoint\\\\S11_09_172.npy'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m=== Batch ===\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_wifi-csi:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_wifi-csi\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\dfine\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    739\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\dfine\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1515\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1513\u001b[0m worker_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info\u001b[38;5;241m.\u001b[39mpop(idx)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1514\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworker_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\dfine\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1550\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[1;34m(self, data, worker_idx)\u001b[0m\n\u001b[0;32m   1548\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[1;32m-> 1550\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\dfine\\Lib\\site-packages\\torch\\_utils.py:750\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    746\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments or otherwise can't\u001b[39;00m\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;66;03m# be constructed, don't try to instantiate since we don't know how to\u001b[39;00m\n\u001b[0;32m    749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"c:\\Users\\Admin\\anaconda3\\envs\\dfine\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\anaconda3\\envs\\dfine\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\anaconda3\\envs\\dfine\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"c:\\Users\\Admin\\OneDrive\\Desktop\\Desktop\\DT-Pose\\feeder\\person_in_wifi_3d.py\", line 129, in __getitem__\n    pose_data = np.array(np.load(item['keypoint_path']))\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\anaconda3\\envs\\dfine\\Lib\\site-packages\\numpy\\lib\\npyio.py\", line 427, in load\n    fid = stack.enter_context(open(os_fspath(file), \"rb\"))\n                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Admin\\\\OneDrive\\\\Desktop\\\\Desktop\\\\DT-Pose\\\\data\\\\person_in_wifi_3d\\\\train_data\\\\keypoint\\\\S11_09_172.npy'\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(\"=== Batch ===\")\n",
    "    print(\"input_wifi-csi:\", batch['input_wifi-csi'].shape)\n",
    "    print(\"output:\", batch['output'].shape)           # pose padded\n",
    "    print(\"person_num:\", batch['person_num'])\n",
    "    print(\"next frame:\", batch['input_wifi-csi_next_frame'].shape)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dfine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
